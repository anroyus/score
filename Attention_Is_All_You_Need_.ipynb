{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOSxD6wh/OppyIg5k4ARlY6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anroyus/score/blob/master/Attention_Is_All_You_Need_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Core PyTorch & TorchText (versions compatible with each other)\n",
        "!pip install torch==2.0.1 torchtext==0.15.2 -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW9TczyuFivr",
        "outputId": "a3cb4e55-a26a-45f0-8785-454809eb8698",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch==2.0.1 in /usr/local/lib/python3.11/dist-packages (2.0.1)\n",
            "Requirement already satisfied: torchtext==0.15.2 in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (4.14.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1) (2.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (2.0.2)\n",
            "Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (0.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (75.2.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.45.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.6.1->torchtext==0.15.2) (2.4.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (2025.7.9)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip freeze torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dVmc8M1B7lsr",
        "outputId": "0a5bc695-cecb-48ef-91d4-f59e7c5431e1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "absl-py==1.4.0\n",
            "accelerate==1.8.1\n",
            "aiofiles==24.1.0\n",
            "aiohappyeyeballs==2.6.1\n",
            "aiohttp==3.11.15\n",
            "aiosignal==1.4.0\n",
            "alabaster==1.0.0\n",
            "albucore==0.0.24\n",
            "albumentations==2.0.8\n",
            "ale-py==0.11.1\n",
            "altair==5.5.0\n",
            "annotated-types==0.7.0\n",
            "antlr4-python3-runtime==4.9.3\n",
            "anyio==4.9.0\n",
            "argon2-cffi==25.1.0\n",
            "argon2-cffi-bindings==21.2.0\n",
            "array_record==0.7.2\n",
            "arviz==0.21.0\n",
            "astropy==7.1.0\n",
            "astropy-iers-data==0.2025.7.7.0.39.39\n",
            "astunparse==1.6.3\n",
            "atpublic==5.1\n",
            "attrs==25.3.0\n",
            "audioread==3.0.1\n",
            "autograd==1.8.0\n",
            "babel==2.17.0\n",
            "backcall==0.2.0\n",
            "backports.tarfile==1.2.0\n",
            "beautifulsoup4==4.13.4\n",
            "betterproto==2.0.0b6\n",
            "bigframes==2.8.0\n",
            "bigquery-magics==0.10.1\n",
            "bleach==6.2.0\n",
            "blinker==1.9.0\n",
            "blis==1.3.0\n",
            "blobfile==3.0.0\n",
            "blosc2==3.5.1\n",
            "bokeh==3.7.3\n",
            "Bottleneck==1.4.2\n",
            "bqplot==0.12.45\n",
            "branca==0.8.1\n",
            "build==1.2.2.post1\n",
            "CacheControl==0.14.3\n",
            "cachetools==5.5.2\n",
            "catalogue==2.0.10\n",
            "certifi==2025.7.9\n",
            "cffi==1.17.1\n",
            "chardet==5.2.0\n",
            "charset-normalizer==3.4.2\n",
            "chex==0.1.89\n",
            "clarabel==0.11.1\n",
            "click==8.2.1\n",
            "cloudpathlib==0.21.1\n",
            "cloudpickle==3.1.1\n",
            "cmake==3.31.6\n",
            "cmdstanpy==1.2.5\n",
            "colorcet==3.1.0\n",
            "colorlover==0.3.0\n",
            "colour==0.1.5\n",
            "community==1.0.0b1\n",
            "confection==0.1.5\n",
            "cons==0.4.6\n",
            "contourpy==1.3.2\n",
            "cramjam==2.10.0\n",
            "cryptography==43.0.3\n",
            "cuda-python==12.6.2.post1\n",
            "cudf-cu12 @ https://pypi.nvidia.com/cudf-cu12/cudf_cu12-25.2.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\n",
            "cudf-polars-cu12==25.2.2\n",
            "cufflinks==0.17.3\n",
            "cuml-cu12==25.2.1\n",
            "cupy-cuda12x==13.3.0\n",
            "curl_cffi==0.11.4\n",
            "cuvs-cu12==25.2.1\n",
            "cvxopt==1.3.2\n",
            "cvxpy==1.6.6\n",
            "cycler==0.12.1\n",
            "cyipopt==1.5.0\n",
            "cymem==2.0.11\n",
            "Cython==3.0.12\n",
            "dask==2024.12.1\n",
            "dask-cuda==25.2.0\n",
            "dask-cudf-cu12==25.2.2\n",
            "dask-expr==1.1.21\n",
            "dataproc-spark-connect==0.8.2\n",
            "datascience==0.17.6\n",
            "datasets==2.14.4\n",
            "db-dtypes==1.4.3\n",
            "dbus-python==1.2.18\n",
            "de_core_news_sm @ https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl#sha256=fec69fec52b1780f2d269d5af7582a5e28028738bd3190532459aeb473bfa3e7\n",
            "debugpy==1.8.0\n",
            "decorator==4.4.2\n",
            "defusedxml==0.7.1\n",
            "diffusers==0.34.0\n",
            "dill==0.3.7\n",
            "distributed==2024.12.1\n",
            "distributed-ucxx-cu12==0.42.0\n",
            "distro==1.9.0\n",
            "dlib==19.24.6\n",
            "dm-tree==0.1.9\n",
            "docstring_parser==0.16\n",
            "docutils==0.21.2\n",
            "dopamine_rl==4.1.2\n",
            "duckdb==1.2.2\n",
            "earthengine-api==1.5.23\n",
            "easydict==1.13\n",
            "editdistance==0.8.1\n",
            "eerepr==0.1.2\n",
            "einops==0.8.1\n",
            "en_core_web_sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl#sha256=1932429db727d4bff3deed6b34cfc05df17794f4a52eeb26cf8928f7c1a0fb85\n",
            "entrypoints==0.4\n",
            "et_xmlfile==2.0.0\n",
            "etils==1.12.2\n",
            "etuples==0.3.9\n",
            "Farama-Notifications==0.0.4\n",
            "fastai==2.7.19\n",
            "fastapi==0.116.0\n",
            "fastcore==1.7.29\n",
            "fastdownload==0.0.7\n",
            "fastjsonschema==2.21.1\n",
            "fastprogress==1.0.3\n",
            "fastrlock==0.8.3\n",
            "ffmpy==0.6.0\n",
            "filelock==3.18.0\n",
            "firebase-admin==6.9.0\n",
            "Flask==3.1.1\n",
            "flatbuffers==25.2.10\n",
            "flax==0.10.6\n",
            "folium==0.19.7\n",
            "fonttools==4.58.5\n",
            "frozendict==2.4.6\n",
            "frozenlist==1.7.0\n",
            "fsspec==2023.9.2\n",
            "future==1.0.0\n",
            "gast==0.6.0\n",
            "gcsfs==2025.3.2\n",
            "GDAL==3.8.4\n",
            "gdown==5.2.0\n",
            "geemap==0.35.3\n",
            "geocoder==1.38.1\n",
            "geographiclib==2.0\n",
            "geopandas==1.0.1\n",
            "geopy==2.4.1\n",
            "gin-config==0.5.0\n",
            "gitdb==4.0.12\n",
            "GitPython==3.1.44\n",
            "glob2==0.7\n",
            "google==2.0.3\n",
            "google-ai-generativelanguage==0.6.15\n",
            "google-api-core==2.25.1\n",
            "google-api-python-client==2.176.0\n",
            "google-auth==2.38.0\n",
            "google-auth-httplib2==0.2.0\n",
            "google-auth-oauthlib==1.2.2\n",
            "google-cloud-aiplatform==1.102.0\n",
            "google-cloud-bigquery==3.34.0\n",
            "google-cloud-bigquery-connection==1.18.3\n",
            "google-cloud-bigquery-storage==2.32.0\n",
            "google-cloud-core==2.4.3\n",
            "google-cloud-dataproc==5.21.0\n",
            "google-cloud-datastore==2.21.0\n",
            "google-cloud-firestore==2.21.0\n",
            "google-cloud-functions==1.20.4\n",
            "google-cloud-iam==2.19.1\n",
            "google-cloud-language==2.17.2\n",
            "google-cloud-resource-manager==1.14.2\n",
            "google-cloud-spanner==3.55.0\n",
            "google-cloud-storage==2.19.0\n",
            "google-cloud-translate==3.21.1\n",
            "google-colab @ file:///colabtools/dist/google_colab-1.0.0.tar.gz\n",
            "google-crc32c==1.7.1\n",
            "google-genai==1.24.0\n",
            "google-generativeai==0.8.5\n",
            "google-pasta==0.2.0\n",
            "google-resumable-media==2.7.2\n",
            "googleapis-common-protos==1.70.0\n",
            "googledrivedownloader==1.1.0\n",
            "gradio==5.31.0\n",
            "gradio_client==1.10.1\n",
            "graphviz==0.21\n",
            "greenlet==3.2.3\n",
            "groovy==0.1.2\n",
            "grpc-google-iam-v1==0.14.2\n",
            "grpc-interceptor==0.15.4\n",
            "grpcio==1.73.1\n",
            "grpcio-status==1.71.2\n",
            "grpclib==0.4.8\n",
            "gspread==6.2.1\n",
            "gspread-dataframe==4.0.0\n",
            "gym==0.25.2\n",
            "gym-notices==0.0.8\n",
            "gymnasium==1.2.0\n",
            "h11==0.16.0\n",
            "h2==4.2.0\n",
            "h5netcdf==1.6.3\n",
            "h5py==3.14.0\n",
            "hdbscan==0.8.40\n",
            "hf-xet==1.1.5\n",
            "hf_transfer==0.1.9\n",
            "highspy==1.11.0\n",
            "holidays==0.76\n",
            "holoviews==1.21.0\n",
            "hpack==4.1.0\n",
            "html5lib==1.1\n",
            "httpcore==1.0.9\n",
            "httpimport==1.4.1\n",
            "httplib2==0.22.0\n",
            "httpx==0.28.1\n",
            "huggingface-hub==0.33.2\n",
            "humanize==4.12.3\n",
            "hyperframe==6.1.0\n",
            "hyperopt==0.2.7\n",
            "ibis-framework==9.5.0\n",
            "idna==3.10\n",
            "imageio==2.37.0\n",
            "imageio-ffmpeg==0.6.0\n",
            "imagesize==1.4.1\n",
            "imbalanced-learn==0.13.0\n",
            "immutabledict==4.2.1\n",
            "importlib_metadata==8.7.0\n",
            "importlib_resources==6.5.2\n",
            "imutils==0.5.4\n",
            "inflect==7.5.0\n",
            "iniconfig==2.1.0\n",
            "intel-cmplr-lib-ur==2025.2.0\n",
            "intel-openmp==2025.2.0\n",
            "ipyevents==2.0.2\n",
            "ipyfilechooser==0.6.0\n",
            "ipykernel==6.17.1\n",
            "ipyleaflet==0.20.0\n",
            "ipyparallel==8.8.0\n",
            "ipython==7.34.0\n",
            "ipython-genutils==0.2.0\n",
            "ipython-sql==0.5.0\n",
            "ipytree==0.2.2\n",
            "ipywidgets==7.7.1\n",
            "itsdangerous==2.2.0\n",
            "jaraco.classes==3.4.0\n",
            "jaraco.context==6.0.1\n",
            "jaraco.functools==4.2.1\n",
            "jax==0.5.2\n",
            "jax-cuda12-pjrt==0.5.1\n",
            "jax-cuda12-plugin==0.5.1\n",
            "jaxlib==0.5.1\n",
            "jeepney==0.9.0\n",
            "jieba==0.42.1\n",
            "Jinja2==3.1.6\n",
            "jiter==0.10.0\n",
            "joblib==1.5.1\n",
            "jsonpatch==1.33\n",
            "jsonpickle==4.1.1\n",
            "jsonpointer==3.0.0\n",
            "jsonschema==4.24.0\n",
            "jsonschema-specifications==2025.4.1\n",
            "jupyter-client==6.1.12\n",
            "jupyter-console==6.1.0\n",
            "jupyter-leaflet==0.20.0\n",
            "jupyter-server==1.16.0\n",
            "jupyter_core==5.8.1\n",
            "jupyter_kernel_gateway @ git+https://github.com/googlecolab/kernel_gateway@b134e9945df25c2dcb98ade9129399be10788671\n",
            "jupyterlab_pygments==0.3.0\n",
            "jupyterlab_widgets==3.0.15\n",
            "jupytext==1.17.2\n",
            "kaggle==1.7.4.5\n",
            "kagglehub==0.3.12\n",
            "keras==3.8.0\n",
            "keras-hub==0.18.1\n",
            "keras-nlp==0.18.1\n",
            "keyring==25.6.0\n",
            "keyrings.google-artifactregistry-auth==1.1.2\n",
            "kiwisolver==1.4.8\n",
            "langchain==0.3.26\n",
            "langchain-core==0.3.68\n",
            "langchain-text-splitters==0.3.8\n",
            "langcodes==3.5.0\n",
            "langsmith==0.4.4\n",
            "language_data==1.3.0\n",
            "launchpadlib==1.10.16\n",
            "lazr.restfulclient==0.14.4\n",
            "lazr.uri==1.0.6\n",
            "lazy_loader==0.4\n",
            "libclang==18.1.1\n",
            "libcudf-cu12 @ https://pypi.nvidia.com/libcudf-cu12/libcudf_cu12-25.2.1-py3-none-manylinux_2_28_x86_64.whl\n",
            "libcugraph-cu12==25.2.0\n",
            "libcuml-cu12==25.2.1\n",
            "libcuvs-cu12==25.2.1\n",
            "libkvikio-cu12==25.2.1\n",
            "libpysal==4.13.0\n",
            "libraft-cu12==25.2.0\n",
            "librosa==0.11.0\n",
            "libucx-cu12==1.18.1\n",
            "libucxx-cu12==0.42.0\n",
            "lightgbm @ file:///tmp/lightgbm/LightGBM/dist/lightgbm-4.5.0-py3-none-linux_x86_64.whl\n",
            "linkify-it-py==2.0.3\n",
            "lit==18.1.8\n",
            "llvmlite==0.43.0\n",
            "locket==1.0.0\n",
            "logical-unification==0.4.6\n",
            "lxml==5.4.0\n",
            "Mako==1.1.3\n",
            "marisa-trie==1.2.1\n",
            "Markdown==3.8.2\n",
            "markdown-it-py==3.0.0\n",
            "MarkupSafe==3.0.2\n",
            "matplotlib==3.10.0\n",
            "matplotlib-inline==0.1.7\n",
            "matplotlib-venn==1.1.2\n",
            "mdit-py-plugins==0.4.2\n",
            "mdurl==0.1.2\n",
            "miniKanren==1.0.3\n",
            "missingno==0.5.2\n",
            "mistune==3.1.3\n",
            "mizani==0.13.5\n",
            "mkl==2025.0.1\n",
            "ml-dtypes==0.4.1\n",
            "mlxtend==0.23.4\n",
            "more-itertools==10.7.0\n",
            "moviepy==1.0.3\n",
            "mpmath==1.3.0\n",
            "msgpack==1.1.1\n",
            "multidict==6.6.3\n",
            "multipledispatch==1.0.0\n",
            "multiprocess==0.70.15\n",
            "multitasking==0.0.11\n",
            "murmurhash==1.0.13\n",
            "music21==9.3.0\n",
            "namex==0.1.0\n",
            "narwhals==1.46.0\n",
            "natsort==8.4.0\n",
            "nbclassic==1.3.1\n",
            "nbclient==0.10.2\n",
            "nbconvert==7.16.6\n",
            "nbformat==5.10.4\n",
            "ndindex==1.10.0\n",
            "nest-asyncio==1.6.0\n",
            "networkx==3.5\n",
            "nibabel==5.3.2\n",
            "nltk==3.9.1\n",
            "notebook==6.5.7\n",
            "notebook_shim==0.2.4\n",
            "numba==0.60.0\n",
            "numba-cuda==0.2.0\n",
            "numexpr==2.11.0\n",
            "numpy==2.0.2\n",
            "nvidia-cublas-cu11==11.10.3.66\n",
            "nvidia-cublas-cu12==12.5.3.2\n",
            "nvidia-cuda-cupti-cu11==11.7.101\n",
            "nvidia-cuda-cupti-cu12==12.5.82\n",
            "nvidia-cuda-nvcc-cu12==12.5.82\n",
            "nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "nvidia-cuda-nvrtc-cu12==12.5.82\n",
            "nvidia-cuda-runtime-cu11==11.7.99\n",
            "nvidia-cuda-runtime-cu12==12.5.82\n",
            "nvidia-cudnn-cu11==8.5.0.96\n",
            "nvidia-cudnn-cu12==9.3.0.75\n",
            "nvidia-cufft-cu11==10.9.0.58\n",
            "nvidia-cufft-cu12==11.2.3.61\n",
            "nvidia-curand-cu11==10.2.10.91\n",
            "nvidia-curand-cu12==10.3.6.82\n",
            "nvidia-cusolver-cu11==11.4.0.1\n",
            "nvidia-cusolver-cu12==11.6.3.83\n",
            "nvidia-cusparse-cu11==11.7.4.91\n",
            "nvidia-cusparse-cu12==12.5.1.3\n",
            "nvidia-cusparselt-cu12==0.6.2\n",
            "nvidia-ml-py==12.575.51\n",
            "nvidia-nccl-cu11==2.14.3\n",
            "nvidia-nccl-cu12==2.21.5\n",
            "nvidia-nvcomp-cu12==4.2.0.11\n",
            "nvidia-nvjitlink-cu12==12.5.82\n",
            "nvidia-nvtx-cu11==11.7.91\n",
            "nvidia-nvtx-cu12==12.4.127\n",
            "nvtx==0.2.12\n",
            "nx-cugraph-cu12 @ https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-25.2.0-py3-none-any.whl\n",
            "oauth2client==4.1.3\n",
            "oauthlib==3.3.1\n",
            "omegaconf==2.3.0\n",
            "openai==1.93.3\n",
            "opencv-contrib-python==4.11.0.86\n",
            "opencv-python==4.11.0.86\n",
            "opencv-python-headless==4.12.0.88\n",
            "openpyxl==3.1.5\n",
            "opt_einsum==3.4.0\n",
            "optax==0.2.5\n",
            "optree==0.16.0\n",
            "orbax-checkpoint==0.11.16\n",
            "orjson==3.10.18\n",
            "osqp==1.0.4\n",
            "packaging==24.2\n",
            "pandas==2.2.2\n",
            "pandas-datareader==0.10.0\n",
            "pandas-gbq==0.29.1\n",
            "pandas-stubs==2.2.2.240909\n",
            "pandocfilters==1.5.1\n",
            "panel==1.7.3\n",
            "param==2.2.1\n",
            "parso==0.8.4\n",
            "parsy==2.1\n",
            "partd==1.4.2\n",
            "pathlib==1.0.1\n",
            "patsy==1.0.1\n",
            "peewee==3.18.2\n",
            "peft==0.16.0\n",
            "pexpect==4.9.0\n",
            "pickleshare==0.7.5\n",
            "pillow==11.2.1\n",
            "platformdirs==4.3.8\n",
            "plotly==5.24.1\n",
            "plotnine==0.14.6\n",
            "pluggy==1.6.0\n",
            "ply==3.11\n",
            "polars==1.21.0\n",
            "pooch==1.8.2\n",
            "portpicker==1.5.2\n",
            "preshed==3.0.10\n",
            "prettytable==3.16.0\n",
            "proglog==0.1.12\n",
            "progressbar2==4.5.0\n",
            "prometheus_client==0.22.1\n",
            "promise==2.3\n",
            "prompt_toolkit==3.0.51\n",
            "propcache==0.3.2\n",
            "prophet==1.1.7\n",
            "proto-plus==1.26.1\n",
            "protobuf==5.29.5\n",
            "psutil==5.9.5\n",
            "psycopg2==2.9.10\n",
            "ptyprocess==0.7.0\n",
            "py-cpuinfo==9.0.0\n",
            "py4j==0.10.9.7\n",
            "pyarrow==18.1.0\n",
            "pyasn1==0.6.1\n",
            "pyasn1_modules==0.4.2\n",
            "pycairo==1.28.0\n",
            "pycocotools==2.0.10\n",
            "pycparser==2.22\n",
            "pycryptodomex==3.23.0\n",
            "pydantic==2.11.7\n",
            "pydantic_core==2.33.2\n",
            "pydata-google-auth==1.9.1\n",
            "pydot==3.0.4\n",
            "pydotplus==2.0.2\n",
            "PyDrive==1.3.1\n",
            "PyDrive2==1.21.3\n",
            "pydub==0.25.1\n",
            "pyerfa==2.0.1.5\n",
            "pygame==2.6.1\n",
            "pygit2==1.18.0\n",
            "Pygments==2.19.2\n",
            "PyGObject==3.42.0\n",
            "PyJWT==2.10.1\n",
            "pylibcudf-cu12 @ https://pypi.nvidia.com/pylibcudf-cu12/pylibcudf_cu12-25.2.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\n",
            "pylibcugraph-cu12==25.2.0\n",
            "pylibraft-cu12==25.2.0\n",
            "pymc==5.23.0\n",
            "pymystem3==0.2.0\n",
            "pynndescent==0.5.13\n",
            "pynvjitlink-cu12==0.7.0\n",
            "pynvml==12.0.0\n",
            "pyogrio==0.11.0\n",
            "pyomo==6.9.2\n",
            "PyOpenGL==3.1.9\n",
            "pyOpenSSL==24.2.1\n",
            "pyparsing==3.2.3\n",
            "pyperclip==1.9.0\n",
            "pyproj==3.7.1\n",
            "pyproject_hooks==1.2.0\n",
            "pyshp==2.3.1\n",
            "PySocks==1.7.1\n",
            "pyspark==3.5.1\n",
            "pytensor==2.31.7\n",
            "pytest==8.3.5\n",
            "python-apt==0.0.0\n",
            "python-box==7.3.2\n",
            "python-dateutil==2.9.0.post0\n",
            "python-louvain==0.16\n",
            "python-multipart==0.0.20\n",
            "python-slugify==8.0.4\n",
            "python-snappy==0.7.3\n",
            "python-utils==3.9.1\n",
            "pytz==2025.2\n",
            "pyviz_comms==3.0.6\n",
            "PyWavelets==1.8.0\n",
            "PyYAML==6.0.2\n",
            "pyzmq==24.0.1\n",
            "raft-dask-cu12==25.2.0\n",
            "rapids-dask-dependency==25.2.0\n",
            "ratelim==0.1.6\n",
            "referencing==0.36.2\n",
            "regex==2024.11.6\n",
            "requests==2.32.3\n",
            "requests-oauthlib==2.0.0\n",
            "requests-toolbelt==1.0.0\n",
            "requirements-parser==0.9.0\n",
            "rich==13.9.4\n",
            "rmm-cu12==25.2.0\n",
            "roman-numerals-py==3.1.0\n",
            "rpds-py==0.26.0\n",
            "rpy2==3.5.17\n",
            "rsa==4.9.1\n",
            "ruff==0.12.2\n",
            "safehttpx==0.1.6\n",
            "safetensors==0.5.3\n",
            "scikit-image==0.25.2\n",
            "scikit-learn==1.6.1\n",
            "scipy==1.15.3\n",
            "scooby==0.10.1\n",
            "scs==3.2.7.post2\n",
            "seaborn==0.13.2\n",
            "SecretStorage==3.3.3\n",
            "semantic-version==2.10.0\n",
            "Send2Trash==1.8.3\n",
            "sentence-transformers==4.1.0\n",
            "sentencepiece==0.2.0\n",
            "sentry-sdk==2.32.0\n",
            "shap==0.48.0\n",
            "shapely==2.1.1\n",
            "shellingham==1.5.4\n",
            "simple-parsing==0.1.7\n",
            "simplejson==3.20.1\n",
            "simsimd==6.5.0\n",
            "six==1.17.0\n",
            "sklearn-compat==0.1.3\n",
            "sklearn-pandas==2.2.0\n",
            "slicer==0.0.8\n",
            "smart_open==7.3.0.post1\n",
            "smmap==5.0.2\n",
            "sniffio==1.3.1\n",
            "snowballstemmer==3.0.1\n",
            "sortedcontainers==2.4.0\n",
            "soundfile==0.13.1\n",
            "soupsieve==2.7\n",
            "soxr==0.5.0.post1\n",
            "spacy==3.8.7\n",
            "spacy-legacy==3.0.12\n",
            "spacy-loggers==1.0.5\n",
            "spanner-graph-notebook==1.1.7\n",
            "Sphinx==8.2.3\n",
            "sphinxcontrib-applehelp==2.0.0\n",
            "sphinxcontrib-devhelp==2.0.0\n",
            "sphinxcontrib-htmlhelp==2.1.0\n",
            "sphinxcontrib-jsmath==1.0.1\n",
            "sphinxcontrib-qthelp==2.0.0\n",
            "sphinxcontrib-serializinghtml==2.0.0\n",
            "SQLAlchemy==2.0.41\n",
            "sqlglot==25.20.2\n",
            "sqlparse==0.5.3\n",
            "srsly==2.5.1\n",
            "stanio==0.5.1\n",
            "starlette==0.46.2\n",
            "statsmodels==0.14.5\n",
            "stringzilla==3.12.5\n",
            "stumpy==1.13.0\n",
            "sympy==1.13.1\n",
            "tables==3.10.2\n",
            "tabulate==0.9.0\n",
            "tbb==2022.2.0\n",
            "tblib==3.1.0\n",
            "tcmlib==1.4.0\n",
            "tenacity==8.5.0\n",
            "tensorboard==2.18.0\n",
            "tensorboard-data-server==0.7.2\n",
            "tensorflow==2.18.0\n",
            "tensorflow-datasets==4.9.9\n",
            "tensorflow-hub==0.16.1\n",
            "tensorflow-io-gcs-filesystem==0.37.1\n",
            "tensorflow-metadata==1.17.2\n",
            "tensorflow-probability==0.25.0\n",
            "tensorflow-text==2.18.1\n",
            "tensorflow_decision_forests==1.11.0\n",
            "tensorstore==0.1.74\n",
            "termcolor==3.1.0\n",
            "terminado==0.18.1\n",
            "text-unidecode==1.3\n",
            "textblob==0.19.0\n",
            "tf-slim==1.1.0\n",
            "tf_keras==2.18.0\n",
            "thinc==8.3.6\n",
            "threadpoolctl==3.6.0\n",
            "tifffile==2025.6.11\n",
            "tiktoken==0.9.0\n",
            "timm==1.0.16\n",
            "tinycss2==1.4.0\n",
            "tokenizers==0.21.2\n",
            "toml==0.10.2\n",
            "tomlkit==0.13.3\n",
            "toolz==0.12.1\n",
            "torch==2.0.1\n",
            "torchao==0.10.0\n",
            "torchaudio @ https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl\n",
            "torchdata==0.6.1\n",
            "torchsummary==1.5.1\n",
            "torchtext==0.15.2\n",
            "torchtune==0.6.1\n",
            "torchvision @ https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl\n",
            "tornado==6.4.2\n",
            "tqdm==4.67.1\n",
            "traitlets==5.7.1\n",
            "traittypes==0.2.1\n",
            "transformers==4.53.1\n",
            "treelite==4.4.1\n",
            "treescope==0.1.9\n",
            "triton==2.0.0\n",
            "tsfresh==0.21.0\n",
            "tweepy==4.15.0\n",
            "typeguard==4.4.4\n",
            "typer==0.16.0\n",
            "types-pytz==2025.2.0.20250516\n",
            "types-setuptools==80.9.0.20250529\n",
            "typing-inspection==0.4.1\n",
            "typing_extensions==4.14.1\n",
            "tzdata==2025.2\n",
            "tzlocal==5.3.1\n",
            "uc-micro-py==1.0.3\n",
            "ucx-py-cu12==0.42.0\n",
            "ucxx-cu12==0.42.0\n",
            "umap-learn==0.5.9.post2\n",
            "umf==0.11.0\n",
            "uritemplate==4.2.0\n",
            "urllib3==2.4.0\n",
            "uvicorn==0.35.0\n",
            "vega-datasets==0.9.0\n",
            "wadllib==1.3.6\n",
            "wandb==0.21.0\n",
            "wasabi==1.1.3\n",
            "wcwidth==0.2.13\n",
            "weasel==0.4.1\n",
            "webcolors==24.11.1\n",
            "webencodings==0.5.1\n",
            "websocket-client==1.8.0\n",
            "websockets==15.0.1\n",
            "Werkzeug==3.1.3\n",
            "widgetsnbextension==3.6.10\n",
            "wordcloud==1.9.4\n",
            "wrapt==1.17.2\n",
            "wurlitzer==3.1.1\n",
            "xarray==2025.3.1\n",
            "xarray-einstats==0.9.1\n",
            "xgboost==2.1.4\n",
            "xlrd==2.0.2\n",
            "xxhash==3.5.0\n",
            "xyzservices==2025.4.0\n",
            "yarl==1.20.1\n",
            "ydf==0.12.0\n",
            "yellowbrick==1.5\n",
            "yfinance==0.2.65\n",
            "zict==3.0.0\n",
            "zipp==3.23.0\n",
            "zstandard==0.23.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Full Transformer Data Pipeline\n",
        "\n",
        "1\tLoad and tokenize Hindi-English\n",
        "2\tBuild vocabularies\n",
        "3\tEncode tokens into tensors\n",
        "4\tCreate dataset + __getitem__\n",
        "5\tcollate_fn for dynamic padding\n",
        "6\tPositional Encoding\n",
        "7\tDefine Transformer model\n",
        "8\tTraining loop\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "FeV6wT5YTbLY",
        "outputId": "e699a84e-53a9-4623-fda3-b89134aa9dee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nFull Transformer Data Pipeline\\n\\n1\\tLoad and tokenize Hindi-English\\n2\\tBuild vocabularies\\n3\\tEncode tokens into tensors\\n4\\tCreate dataset + __getitem__\\n5\\tcollate_fn for dynamic padding\\n6\\tPositional Encoding\\n7\\tDefine Transformer model\\n8\\tTraining loop\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face datasets for loading IITB Hindi-English corpus\n",
        "!pip install -q datasets"
      ],
      "metadata": {
        "id": "BLw-bHXhQ6Xu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix for Hugging Face + fsspec caching issue (critical!)..I ran into various local caching issues earlier due to higher fsspec versions..2025.X\n",
        "!pip install fsspec==2023.9.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdHKmSiPmI6L",
        "outputId": "c48b59b3-aaa5-4335-9969-c89758ccf9d1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fsspec==2023.9.2 in /usr/local/lib/python3.11/dist-packages (2023.9.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Step 1: Load Dataset\n",
        "# Load IITB English to Hindi data sets... we can always build our own later.\n",
        "from datasets import load_dataset\n",
        "\n",
        "raw_dataset = load_dataset(\"cfilt/iitb-english-hindi\", split=\"train\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RQnXvQAiPYp",
        "outputId": "9b7d7fac-f9d8-479b-dd42-2dab9774940b",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test code: Check sample\n",
        "print(raw_dataset[0])\n",
        "print(raw_dataset.features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hF8uKbj2nLUF",
        "outputId": "002e2845-ec8c-4393-8ee2-1d745be5dc8e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'translation': {'en': 'Give your application an accessibility workout', 'hi': 'अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें'}}\n",
            "{'translation': {'en': Value(dtype='string', id=None), 'hi': Value(dtype='string', id=None)}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from datasets import load_dataset\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "wWhfGraGFKuL",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def hindi_tokenizer(text):\n",
        "    return [word.strip(\"।\") for word in text.strip().split()]\n",
        "\n",
        "HI_TOKENIZER = hindi_tokenizer"
      ],
      "metadata": {
        "id": "vke0W0eB1vVd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Hindi Tokenizer\n",
        "print(hindi_tokenizer(\"भारत एक सुंदर देश है।\"))  # ['भारत', 'एक', 'सुंदर', 'देश', 'है']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBNi08gs2_Rs",
        "outputId": "c19ac317-254f-4340-fe6f-652958cb8b04"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['भारत', 'एक', 'सुंदर', 'देश', 'है']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test validation\n",
        "for i in range(9999):\n",
        "  text = raw_dataset[i][\"translation\"][\"hi\"]\n",
        "  if 'भारत' in text:\n",
        "      print(f\"{i}: {text}\")\n"
      ],
      "metadata": {
        "id": "gP3y4u5n5CDL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define Tokenizers\n",
        "# Using torchtext's get_tokenizer. For Hindi, we’ll use a whitespace tokenizer for now:\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "# Tokenizers\n",
        "EN_TOKENIZER = get_tokenizer(\"basic_english\")\n",
        "# HI_TOKENIZER = lambda x: x.strip().split()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mINmAnQoVWZ9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Yield Tokens for Vocabulary Building\n",
        "# We need iterators that tokenize sentences and yield tokens for vocab building:\n",
        "def yield_tokens(data_iter, tokenizer, lang):\n",
        "    for example in data_iter:\n",
        "        yield tokenizer(example[\"translation\"][lang])\n",
        "\n"
      ],
      "metadata": {
        "id": "gtP192v4Vw5K"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Step 4:Build Vocabularies\n",
        "# We’ll create vocabularies with special tokens like <unk>, <pad>, <bos>, <eos>:\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "specials = ['<unk>', '<pad>', '<bos>', '<eos>']\n",
        "\n",
        "HI_VOCAB = build_vocab_from_iterator(yield_tokens(raw_dataset, HI_TOKENIZER, \"hi\"),\n",
        "                                    specials=specials, max_tokens=25000)\n",
        "HI_VOCAB.set_default_index(HI_VOCAB['<unk>'])\n",
        "\n",
        "\n",
        "EN_VOCAB = build_vocab_from_iterator(yield_tokens(raw_dataset, EN_TOKENIZER, \"en\"),\n",
        "                                     specials=specials, max_tokens=25000)\n",
        "EN_VOCAB.set_default_index(EN_VOCAB['<unk>'])\n",
        "\n"
      ],
      "metadata": {
        "id": "5PR9iaxzPt2Z"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "print(HI_VOCAB.get_itos()[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAIuuag-4CXY",
        "outputId": "d238f4d0-b52e-4c8d-aeb9-8e272854d17e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<unk>', '<pad>', '<bos>', '<eos>', 'के', 'और', 'में', 'है', 'की', 'से', 'को', 'का', 'कि', 'पर', 'लिए', 'एक', 'हैं', 'तो', 'नहीं', 'जो', 'भी', 'ने', 'यह', 'हो', 'किया', 'ही', 'कर', 'इस', 'या', 'वह', 'करने', 'है,', 'अपने', 'न', 'तुम', 'वे', 'तथा', 'कुछ', 'किसी', 'गया', 'था', 'कोई', 'हम', 'उनके', 'साथ', 'रूप', 'द्वारा', 'है.', 'करते', 'लोगों', 'जब', 'लोग', 'दिया', 'भारत', 'तक', 'ये', 'जाता', 'फिर', 'उसके', 'थे', 'क्या', 'अल्लाह', 'ख़ुदा', 'अपनी', 'उस', 'जा', 'करना', 'रहे', 'उन्हें', 'करता', 'मैं', 'उन', 'होता', 'उसे', 'रहा', 'हुए', 'करें', 'आप', 'सकता', 'बहुत', 'समय', 'उन्होंने', 'कहा', 'अधिक', 'वाले', 'बात', 'दो', 'हमने', 'तरह', 'पहले', 'उनकी', 'तुम्हारे', 'यदि', 'पास', 'बाद', 'हुआ', 'थी', 'सभी', 'ऐसे', 'हमारे', 'ओर', 'होने', 'हैं,', 'गए', 'दिन', 'गई', 'अन्य', 'इसके', 'काम', 'सकते', 'प्रकार', 'उसकी', 'प्राप्त', 'दी', 'किए', 'होती', 'इन', 'एवं', 'ऐसा', 'कार्य', 'जाने', 'राष्ट्रपति', 'क्षेत्र', 'अगर', 'होगा', 'भारतीय', '(_', 'मुझे', 'सरकार', 'चाहिए', 'उसने', 'कारण', 'अब', 'देश', 'विकास', 'दोनों', 'लेकिन', 'तुम्हें', 'नाम', 'जाती', 'ऐसी', 'कम', 'सब', 'हुई', 'दे', 'राज्य', 'व्यक्ति', 'प्रदान', 'व', 'लिया', 'बारे', 'जिस', 'वर्ष', 'होते', 'ले', 'करो', 'रही', 'वाला', 'हर', 'हैं.', 'बीच', 'उसका', 'जैसे', 'में,', 'ईमान', 'जीवन', 'उनका', 'केवल', 'मेरे', 'हमें', 'जिसमें', 'आ', 'वही', 'ताकि', 'सबसे', 'उपयोग', 'वाली', 'अपना', 'इसे', 'उसी', 'देने', 'हमारी', 'प्रति', 'पैदा', 'बड़ा', 'इसमें', 'जाना', 'प्रयोग', 'देता', 'जाते', 'जिसे', 'धारा', 'अथवा', 'राष्ट्रीय', 'बना', 'श्री', 'रब', 'उससे', 'आज', 'परवरदिगार', 'भाग', 'उनसे', 'क्योंकि', 'तब', 'कहा,', 'करती', 'बार', 'दूसरे', 'चीज़', 'सकती', 's', 'उनमें', 'जाएगा', 'स्थान', 'सहायता', 'बस', 'शामिल', 'मेरी', 'सूचना', 'हूँ', 'बड़ी', 'कई', '(ऐ', 'स्थिति', 'संदेश', 'इसलिए', 'इससे', 'होना', 'था,', 'कह', 'बल्कि', 'कहते', 'विभिन्न', 'विशेष', 'पूर्व', 'आगे', 'लिये', 'शिक्षा', 'यहाँ', 'अधीन', 'जहां', 'हो,', 'उनको', 'यही', 'जिन', 'से)', 'है?', 'इसका', 'ऐ', '1', 'जिसके', 'रसूल', 'योजना', 'पता', 'जाए', 'दौरान', 'संबंध', 'तरफ', 'उसमें', 'सामने', 'कभी', 'आधार', 'फ़ाइल', 'करके', 'उपलब्ध', 'संबंधित', 'देते', 'तैयार', 'पूरा', 'बाहर', 'प्रत्येक', 'चाहते', 'हूं', 'इसी', 'सेवा', 'आपको', 'नई', 'सूची', 'याद', 'अनुसार', 'अवसर', 'पूरी', 'विषय', 'रसूल)', 'जारी', 'तुम्हारी', 'आपके', 'बनाने', 'निर्माण', 'मार्ग', 'बड़े', 'होगी', 'मे', 'विचार', 'निश्चय', 'अच्छा', 'अनेक', 'रखा', 'स्वयं', 'ऊपर', 'चाहता', 'में)', 'संख्या', 'पानी', 'महत्वपूर्ण', 'घर', 'मुख्य', 'हेतु', 'प्रक्रिया', 'तुम्हारा', 'शुरू', 'विश्व', '&', 'खुदा', 'जिसने', 'तीन', 'गये', 'यहां', 'मैंने', 'दुनिया', 'स्तर', 'होंगे', 'बेशक', 'ध्यान', 'करेगा', 'सामान्य', 'नीचे', 'जिसका', 'लगे', 'क्षेत्रों', 'बन', 'किन्तु', 'जबकि', 'आदि', 'भवन', 'मदद', 'बिना', 'ज़मीन', 'बनाया', 'करे', 'हाथ', 'लेकर', 'अलग', 'धरती', 'माध्यम', 'उच्च', 'पीछे', 'है;', 'प्रतिशत', 'प्रमुख', 'अधिकार', 'भाषा', 'अतः', 'प्रौद्योगिकी', 'जी', 'ठीक', 'इसकी', 'मगर', 'किताब', 'देना', 'बैंक', 'आर्थिक', 'देखा', 'शक्ति', 'लगा', 'ज्ञान', 'तुमसे', 'इनकार', 'लाभ', 'दिए', 'अज़ाब', 'ज्यादा', 'कंपनी', 'थे,', 'गयी', 'काफी', 'सारे', 'लगभग', 'दूर', 'जनता', 'स्पष्ट', 'लागू', 'अधिनियम,', 'होकर', 'मेरा', 'आने', 'जानकारी', 'रखते', 'जानता', 'विश्वास', 'अर्थ', 'अवधि', 'वहां', 'हों', 'मामले', 'स्थापित', 'कार्यक्रम', 'कृषि', 'सिवा', 'आवश्यक', 'अच्छे', 'जैसा', 'अभी', 'आय', 'अच्छी', 'चाहे', 'देख', 'उसको', 'पूर्ण', 'कोर्इ', 'रह', 'फ़ोल्डर', 'जिससे', 'करेंगे', 'प्रयास', 'मूल', 'बराबर', 'कहीं', '#44;', 'प्रणाली', 'रहने', 'की)', 'अधिनियम', 'परिवर्तन', 'आदेश', 'क्यों', 'था.', 'आवश्यकता', 'समाज', 'उत्तर', 'देशों', 'सके', 'प्रभाव', 'तुमने', 'कहने', 'रहते', 'चुके', 'नए', 'क़ौम', 'तू', '2', 'खंड', 'छोड़', 'नया', 'समूह', 'जगह', 'जानते', 'विज्ञान', 'उद्योग', 'माल', 'रखने', 'नीति', '', 'लेने', '%', 'सुरक्षा', 'अंतर्गत', 'यातना', 'कुल', 'सामाजिक', 'स्वीकार', 'समान', 'है)', ':', 'आया', 'उपधारा', 'वर्षों', 'जिन्हें', 'रंग', 'वर्तमान', 'वालों', 'चुका', 'मूल्य', 'नहीं,', 'a', 'सही', 'सिर्फ', 'उत्पादन', 'अतिरिक्त', 'तुमको', 'दूसरी', 'धर्म', 'पत्र', 'संपर्क', 'ऋण', 'अनुसंधान', 'क्षमता', 'परियोजना', 'जमा', 'नही', 'हमारा', 'उद्देश्य', '_', 'वहाँ', 'वास्तव', 'आकार', 'kgm', 'परन्तु', 'राशि', 'मिल', 'हुक्म', 'लिए,', 'है।\"', 'प्रस्तुत', 'जिनके', 'न्यायालय', 'यात्रा', 'बेहतर', 'सहयोग', 'आए', 'सी', 'मूसा', 'देखते', 'निर्धारित', 'जिन्होंने', 'विकसित', 'से,', 'बच्चों', 'सदस्य', 'हमेशा', 'शरीर', 'जाएगी', 'विभाग', 'अधिकारी', 'स्थित', 'प्रधानमंत्री', 'समझ', 'सभा', 'वास्ते', 'व्यापार', 'लाए', 'कैसे', 'वक्त', 'परिणाम', 'रात', 'जल', 'व्यवस्था', 'स्थानीय', 'दें', 'बदला', 'साल', 'विदेशी', 'name', 'इतिहास', 'कौन', '-', 'दर', 'बनाए', 'चार', 'आग', 'तौर', 'स्वास्थ्य', 'अंतिम', 'सुधार', 'दिल्ली', 'हाल', 'तहत', 'थी,', 'परंतु', 'निश्चित', 'लगता', 'दिल', 'निर्णय', 'केंद्र', 'सीमा', 'ली', 'चल', 'जिसकी', 'विधि', 'समाप्त', 'मौजूद', 'किया,', 'विशिष्ट', 'मन', 'मानव', 'युद्ध', '”', 'रहता', 'निर्धारण', 'किये', 'छवि', 'विश्वविद्यालय', 'आधारित', 'संबंधी', 'राष्ट्र', 'निर्दिष्ट', 'जान', 'निस्संदेह', 'संयुक्त', 'दृष्टि', 'वर्ग', 'हैं?', 'वित्तीय', 'खोज', 'भूमिका', 'माना', 'आरंभ', 'नाज़िल', 'प्रवेश', 'भेजा', 'प्रदेश', 'हिस्सा', 'पहली', 'लाख', 'डर', 'नियंत्रण', 'पिछले', 'शब्द', 'सहायक', 'भर', 'स्थापना', 'अनुमति', 'दिनों', 'प्रथम', 'उत्पन्न', 'रखता', 'वस्तु', 'मंदिर', 'सक्रिय', 'राह', 'संविधान', 'मनुष्य', 'देगा', 'मध्य', 'वृद्धि', 'पाया', 'भूमि', 'लोक', 'उदाहरण', 'निवेश', 'परिवार', 'किस', 'जहाँ', 'सुनिश्चित', 'कृपया', 'सत्य', 'लक्ष्य', 'बंद', 'समर्थन', 'के)', 'अनुभव', 'छोटे', '(और', 'हक़', 'उनपर', 'सरकारी', 'इस्तेमाल', 'संस्थान', 'लेते', 'वापस', 'लो', 'जैसी', 'भीतर', 'भुगतान', 'आसमान', 'समिति', 'प्रारंभ', '(1)', 'अध्ययन', 'आदमी', 'थीं', 'प्रबंधन', 'मंत्रालय', '3', 'कर्म', 'बाजार', 'पूरे', 'अत्यन्त', 'मामलों', 'सार्वजनिक', 'सिंह', 'मृत्यु', 'दिशा', 'मान', 'खाता', 'विंडो', 'काल', 'करें.', 'यक़ीनन', 'योग्य', 'मालूम', 'बच्चे', 'मात्रा', 'अंत', 'खाते', 'प्रशिक्षण', 'बातों', 'डाक', 'जरूरत', 'सर्वर', 'की,', 'खेल', 'संघ', 'विकल्प', 'आम', 'प्रगति', 'जिनमें', 'को,', 'पड़े', 'पर,', 'भाई', 'सहित', 'उचित', 'डाल', 'बदल', 'शख्स', 'प्रकृति', 'वो', 'त्रुटि', 'सक्षम', 'देखने', 'इतना', 'बातें', \"'%\", 'ऊर्जा', 'राज्यों', 'राष्ट्रपति,', 'संसद', 'विस्तार', 'बढ़', 'सदस्यों', 'करोड़', 'साहित्य', 'समस्या', 'कोशिश', 'शहर', 'केंद्रीय', 'जांच', 'व्यापक', 'वेबसाइट', 'जाओ', 'ग्रामीण', 'बयान', 'इनमें', '(optional,', 'सत्ता', 'विवरण', 'दिखाई', 'मिला', 'कमी', 'देती', 'प्रश्न', 'हैं)', 'मुँह', 'संगठन', 'दक्षिण', 'सामग्री', 'नदी', 'पश्चात', 'वजह', 'खुद', 'नियम', 'गांधी', 'शायद', 'आई', 'आयतों', 'लिखा', 'किंतु', 'योगदान', 'पहचान', 'पाठ', 'महान', 'मंत्री', 'शासन', 'not', 'पुनः', 'कानून', 'पूंजी', 'प्रकट', 'रक्षा', 'जाएँ', 'औद्योगिक', 'चयनित', 'translation)', 'आधुनिक', 'जन्म', 'दिखाएँ', 'निजी', 'मै', 'कांग्रेस', 'पद', 'बदले', 'संभव', '\"ऐ', 'दृश्य', 'प्रेम', 'आयोजित', 'फाइल', \"''\", 'मांग', 'रोग', 'प्रभावित', 'भविष्य', 'सफल', 'स्वतंत्रता', 'राजा', 'रखना', 'व्यक्तियों', 'रहो', 'लें', 'वास्तविक', 'जवाब', 'need', 'रिपोर्ट', 'does', 'probably', 'निर्यात', 'पालन', 'प्रकाशित', 'समारोह', 'बल', 'अवश्य', 'हासिल', '\"', 'हैः', 'मत', 'राम', 'डिस्क', 'रुप', 'दशा', 'निम्नलिखित', 'आवेदन', 'दो,', 'पुरस्कार', 'पृष्ठ', 'बने', 'अमेरिका', 'क्रिया', 'चले', 'साफ', 'पर्याप्त', 'कि)', 'को)', 'समुदाय', 'धन', 'रख', 'मुखर्जी', 'चुकी', 'प्रकाश', 'स्रोत', 'संदर्भ', 'आपकी', 'चला', 'कला', 'निर्धारिती', 'ढंग', 'भिन्न', '(के', 'दूसरा', 'लाने', 'जाएँगे', 'राजनीतिक', 'का)', 'सुविधा', 'केन्द्रीय', 'पड़ा', 'बुरा', 'प्रस्ताव', 'पुस्तक', 'पक्ष', 'सफलता', 'लागत', 'व्यक्त', 'तरीके', 'सुरक्षित', '5', 'अलावा', 'सेवाओं', 'यद्यपि', \"s'\", 'शब्दों', 'निधि', 'बनी', 'पेश', 'कहना', '10', 'उपस्थित', 'बंगाल', 'भारी', 'सीधे', 'सेट', 'सेना', 'आता', 'कार्यालय', 'प्राकृतिक', 'विरुद्ध', 'बधाई', 'सन्', 'निकल', 'महिला', 'महत्व', 'प्रभावी', 'गति', 'निकट', 'अगले', 'संस्था', 'हुए,', 'गर्इ', 'संस्कृति', 'आयात', 'प्रमाणपत्र', 'चर्चा', 'दिवस', '<s>', 'रहेंगे', 'संरचना', 'रकम', 'अता', 'मुक्त', 'संसाधन', 'ऑफ', 'उत्पाद', 'मुद्रा', 'गया.', 'डाला', 'आख़िरत', 'रखी', 'होगा,', 'चुनें', 'बोर्ड', 'संरक्षण', 'आपका', 'सका', 'बन्दों', 'भी,', 'आशा', 'बैठक', 'पश्चिम', 'आयोग', 'तुममें', 'आते', 'राय', 'धार्मिक', 'प्रणब', 'पाने', 'करनी', 'भावना', 'उन्हीं', 'ज़रूर', 'पुरुष', 'नमाज़', 'थी.', 'नेटवर्क', 'बाज़', 'खड़े', 'तंत्र', 'आकाश', 'बताया', 'स्कूल', 'वैज्ञानिक', 'तुमपर', 'मजबूत', 'चाहिये', 'लेना', 'वीडियो', 'संगीत', 'केन्द्र', 'जरूरी', 'दिलों', 'वर्णन', 'कदम', 'हिदायत', 'सम्मेलन', 'उठा', 'प्रदर्शन', 'समझते', 'प्राचीन', 'भोजन', 'पहुंच', 'सीमित', 'जल्दी', 'अधिकांश', 'अधिकारियों', 'कार्यों', 'संपत्ति', 'सारी', 'बैठे', 'ब्रिटिश', 'मिलता', 'रहती', 'तुलना', 'करनेवालों', 'बिल्कुल', 'शताब्दी', 'रखे', 'अपेक्षा', 'निर्मित', 'संस्थानों', 'आत्मा', 'अवस्था', 'चयन', 'आती', 'छोड़कर', 'शक', 'पहुँचा', 'सच्चे', 'आपने', 'उपकरण', 'रखो', 'लाल']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "print('भारत' in HI_VOCAB.get_itos())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayetUbWb4Svd",
        "outputId": "9642a14f-68c6-403d-c569-5a51d05eb21f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "for idx, token in enumerate(HI_VOCAB.get_itos()):\n",
        "    if token == 'भारत':\n",
        "        print(f\"'भारत' found at index {idx}\")"
      ],
      "metadata": {
        "id": "hKlP2gk74p6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98a92813-9059-447e-a3f6-e1c55b8cdab4"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'भारत' found at index 53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save vocab\n",
        "import pickle\n",
        "\n",
        "with open(\"hi_vocab.pkl\", \"wb\") as f:\n",
        "    pickle.dump(HI_VOCAB, f)\n",
        "\n",
        "with open(\"en_vocab.pkl\", \"wb\") as f:\n",
        "    pickle.dump(EN_VOCAB, f)\n"
      ],
      "metadata": {
        "id": "qiNnD9YJ55Fg"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Not relevant for my current hindi-to-eng translation\n",
        "# Install necessary packages (torchtext and spacy)\n",
        "!pip install spacy --quiet\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm\n",
        "\n",
        "import spacy\n",
        "spacy_de = spacy.load(\"de_core_news_sm\")\n",
        "spacy_en = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TF7AeWvFDmwz",
        "outputId": "af2ea889-6681-49ad-cc7f-8dbd0b6cb5d3",
        "collapsed": true
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting de-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is not linked to hindi-engligh. This is for dutch\n",
        "\n",
        "def tokenize_de(text):\n",
        "    return [tok.text.lower() for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "t1= tokenize_de(\"Cow is a sacred animal in hindu tradition\")\n",
        "t1_d = spacy_en(\"Cow is a sacred animal in hindu tradition\")\n",
        "t2 = tokenize_de(\"Muslims eat Cow meat\")\n",
        "t3 = tokenize_de(\"Cow milk is popular\")"
      ],
      "metadata": {
        "id": "tF5slUY4GzWa"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IUwtmcwfri5X"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "token.text → the word itself\n",
        "token.pos_ → part of speech (noun, verb, etc.)\n",
        "token.dep_ → syntactic relation (subject, object, etc.)\n",
        "token.lemma_ → base form (e.g., \"running\" → \"run\")\n",
        "doc.ents → named entities (like \"Amazon\", \"Las Vegas\")\n",
        "doc.sents → sentences (if multiple)\n",
        "'''\n",
        "\n",
        "for token in t1_d:\n",
        "    print(f\"{token.text:15} | POS: {token.pos_:10} | DEP: {token.dep_:10} | Lemma: {token.lemma_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tCe3tdTeHXiM",
        "outputId": "1158ea92-fcb6-45bb-e987-9cdafca5f3e9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cow             | POS: NOUN       | DEP: nsubj      | Lemma: cow\n",
            "is              | POS: AUX        | DEP: ROOT       | Lemma: be\n",
            "a               | POS: DET        | DEP: det        | Lemma: a\n",
            "sacred          | POS: ADJ        | DEP: amod       | Lemma: sacred\n",
            "animal          | POS: NOUN       | DEP: attr       | Lemma: animal\n",
            "in              | POS: ADP        | DEP: prep       | Lemma: in\n",
            "hindu           | POS: NOUN       | DEP: compound   | Lemma: hindu\n",
            "tradition       | POS: NOUN       | DEP: pobj       | Lemma: tradition\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing the vocab\n",
        "print(f\"Hindi vocab size: {len(HI_VOCAB)}\")\n",
        "print(f\"English vocab size: {len(EN_VOCAB)}\")\n",
        "\n",
        "print(\"Sample Hindi tokens:\", HI_VOCAB.get_itos()[:10])\n",
        "print(\"Sample English tokens:\", EN_VOCAB.get_itos()[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvunMkDoTj0y",
        "outputId": "b9d98fe9-acb6-4f91-a050-ea252eba7473"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hindi vocab size: 25000\n",
            "English vocab size: 25000\n",
            "Sample Hindi tokens: ['<unk>', '<pad>', '<bos>', '<eos>', 'के', 'और', 'में', 'है', 'की', 'से']\n",
            "Sample English tokens: ['<unk>', '<pad>', '<bos>', '<eos>', 'the', '.', ',', 'of', 'and', 'to']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing vocab\n",
        "test_sent = \"भारत एक सुंदर देश है\"\n",
        "tokens = HI_TOKENIZER(test_sent)\n",
        "print(\"Tokens:\", tokens)\n",
        "indices = [HI_VOCAB[token] for token in tokens]\n",
        "print(\"Indices:\", indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmE8zGO91CjR",
        "outputId": "ef276717-9dc2-4e3d-aef9-a3224bafaf15"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['भारत', 'एक', 'सुंदर', 'देश', 'है']\n",
            "Indices: [53, 15, 1773, 133, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "print('भारत' in HI_VOCAB.get_itos())  # Should be True now\n",
        "print('सुंदर' in HI_VOCAB.get_itos())\n",
        "print('देश' in HI_VOCAB.get_itos())\n",
        "print(HI_VOCAB.get_itos()[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkUP1DIt1UBV",
        "outputId": "8e32d91c-e878-4a05-813f-a37dc02f05eb"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "['<unk>', '<pad>', '<bos>', '<eos>', 'के', 'और', 'में', 'है', 'की', 'से', 'को', 'का', 'कि', 'पर', 'लिए', 'एक', 'हैं', 'तो', 'नहीं', 'जो', 'भी', 'ने', 'यह', 'हो', 'किया', 'ही', 'कर', 'इस', 'या', 'वह', 'करने', 'है,', 'अपने', 'न', 'तुम', 'वे', 'तथा', 'कुछ', 'किसी', 'गया', 'था', 'कोई', 'हम', 'उनके', 'साथ', 'रूप', 'द्वारा', 'है.', 'करते', 'लोगों']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 5: Encode with <bos> and <eos>, and Convert to Tensor\n",
        "# We’ll define a function to:\n",
        "\n",
        "# Tokenize the sentence\n",
        "\n",
        "# Add <bos> and <eos>\n",
        "\n",
        "# Convert tokens to vocab indices\n",
        "\n",
        "# Return a PyTorch tensor\n",
        "\n",
        "# python\n",
        "# Copy\n",
        "# Edit\n",
        "\n",
        "BOS_IDX = EN_VOCAB['<bos>']\n",
        "EOS_IDX = EN_VOCAB['<eos>']\n",
        "PAD_IDX = EN_VOCAB['<pad>']\n",
        "\n",
        "def encode(text, tokenizer, vocab):\n",
        "    #return [vocab['<bos>']] + vocab(tokenizer(text)) + [vocab['<eos>']]\n",
        "    return [vocab['<bos>']] + [vocab[token] for token in tokenizer(text)] + [vocab['<eos>']]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X2WzYnHoFQXd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 6: Build PyTorch Dataset Wrapper\n",
        "# We’ll define a TranslationDataset that will return tensor pairs: (src_tensor, tgt_tensor).\n",
        "class TranslationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, src_lang, tgt_lang, src_tokenizer, tgt_tokenizer, src_vocab, tgt_vocab):\n",
        "        self.data = data\n",
        "        self.src_lang = src_lang\n",
        "        self.tgt_lang = tgt_lang\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        src_text = item['translation'][self.src_lang]\n",
        "        tgt_text = item['translation'][self.tgt_lang]\n",
        "\n",
        "        src_tensor = torch.tensor(encode(src_text, self.src_tokenizer, self.src_vocab), dtype=torch.long)\n",
        "        tgt_tensor = torch.tensor(encode(tgt_text, self.tgt_tokenizer, self.tgt_vocab), dtype=torch.long)\n",
        "\n",
        "        return src_tensor, tgt_tensor\n"
      ],
      "metadata": {
        "id": "rjLa-P4rPFfy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Creating the dataset class\n",
        "Custom TranslationDataset to return input/output pairs\n",
        "Transformers require:\n",
        " - An input sequence (e.g., Hindi sentence)\n",
        " - A target sequence (e.g., English translation)\n",
        " - Both converted into tensors\n",
        " - and special tokens added: <bos> at the start, <eos> at the end\n",
        "\n",
        "\n",
        "So we will tokenize each sentence\n",
        "Encode each token as an index from the vocab\n",
        "Wrap it up in a PyTorch-friendly format\n"
      ],
      "metadata": {
        "id": "431efwC3Eegm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, data, src_tokenizer, tgt_tokenizer, src_vocab, tgt_vocab):\n",
        "        self.data = data\n",
        "        self.src_tokenizer = src_tokenizer\n",
        "        self.tgt_tokenizer = tgt_tokenizer\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def encode(self, text, tokenizer, vocab):\n",
        "        tokens = tokenizer(text.lower().strip())\n",
        "        return [vocab['<bos>']] + [vocab[token] for token in tokens] + [vocab['<eos>']]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        example = self.data[idx]\n",
        "        src_text = example['translation']['hi']\n",
        "        tgt_text = example['translation']['en']\n",
        "\n",
        "        src_tensor = torch.tensor(self.encode(src_text, self.src_tokenizer, self.src_vocab), dtype=torch.long)\n",
        "        tgt_tensor = torch.tensor(self.encode(tgt_text, self.tgt_tokenizer, self.tgt_vocab), dtype=torch.long)\n",
        "\n",
        "        return src_tensor, tgt_tensor\n"
      ],
      "metadata": {
        "id": "YYukLmy756Br"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test code for Step 5\n",
        "dataset = TranslationDataset(\n",
        "    data=raw_dataset,\n",
        "    src_tokenizer=HI_TOKENIZER,\n",
        "    tgt_tokenizer=EN_TOKENIZER,\n",
        "    src_vocab=HI_VOCAB,\n",
        "    tgt_vocab=EN_VOCAB\n",
        ")\n",
        "\n",
        "src_tensor, tgt_tensor = dataset[0]\n",
        "print(src_tensor)  # [<bos>, id1, id2, ..., <eos>]\n",
        "print(tgt_tensor)  # [<bos>, id1, id2, ..., <eos>]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PxeuWTOFAlH",
        "outputId": "d5389849-7324-4afd-89e6-c8106a512d30"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([    2,    32,  1223,    10, 22171,  5950,    11,   368,   538,     3])\n",
            "tensor([   2,  173,   50,  424,   45, 6507,    0,    3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Collate function and batching (handling variable-length sequences with padding)\n",
        "\"\"\"\n",
        "Problem Statement:\n",
        "Sentences have different lengths.\n",
        "\n",
        "We need batches of uniform shape (e.g., [batch_size, max_len]).\n",
        "\n",
        "Solution:\n",
        "The collate function:\n",
        "\n",
        "Pads each sentence in the batch to the length of the longest sentence.\n",
        "\n",
        "Returns padded tensors + lengths (optional).\n",
        "\n",
        "Also creates masks (for padding).\n",
        "A visual interpretation would be that, we are shaping all sequences in the batch into a same size rectangle.\n",
        "\n",
        "\n",
        "e.g. Unequal length token sequences below\n",
        "[\"<bos>\", 12, 18, 92, \"<eos>\"]. --> length : 5\n",
        "[\"<bos>\", 45, 63, \"<eos>\"]  --> length : 4\n",
        "[\"<bos>\", 89, 12, 77, 23, \"<eos>\"]  --> length : 6\n",
        "\n",
        "After Padding:\n",
        "\n",
        "[\n",
        " [ <bos>, 12, 18, 92, <eos>, <pad> ],  --> length : 6\n",
        " [ <bos>, 45, 63, <eos>, <pad>, <pad> ],  --> length : 6\n",
        " [ <bos>, 89, 12, 77, 23, <eos> ].  --> length : 6\n",
        "]\n",
        "\n",
        "DURING TRAINING :\n",
        "Notes: We should use dynamic padding within each batch\n",
        "So if a batch has lengths: [6, 12, 10], we pad to 12 for that batch only\n",
        "This is memory-efficient and doesn't require hardcoding a global max length\n",
        "\n",
        "\n",
        "DURING INFERENCE :\n",
        "At Inference Time or in Model Definition:\n",
        "This is where max sequence length matters.\n",
        "\n",
        "Transformer models must know:\n",
        " - How many positions to encode (in positional encodings)\n",
        " - What shape to expect during attention computation\n",
        " - So here’s what we typically do:\n",
        "        Choose a Safe Maximum Sequence Length\n",
        "Strategy\tMax Length:\n",
        "Observe our training set.\n",
        "Take the 95th percentile length — e.g., 60 tokens\n",
        "Add a buffer\tE.g., use max_len = 128 even if your training set only goes up to 60\n",
        "Very long sequences\tUse 512 or 1024 (like BERT), but that increases memory cost\n",
        "\n",
        "MAX_SEQ_LEN = 128  # Set by you\n",
        "\n",
        "positional_encoding = PositionalEncoding(\n",
        "    d_model=EMBED_DIM,\n",
        "    max_len=MAX_SEQ_LEN\n",
        ")\n",
        "\n",
        "Future-Proofing Tips:\n",
        "Train with dynamic padding, but define positional encodings with a large enough max_len (e.g., 256 or 512)\n",
        "\n",
        "Monitor real-world inference traffic — log token lengths\n",
        "\n",
        "For production: set MAX_LEN = 2x the typical input length\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Take a batch of (src_tensor, tgt_tensor) pairs\n",
        "# Pad them dynamically\n",
        "# Return padded source and target tensors\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "SRC_PAD_IDX = HI_VOCAB['<pad>']\n",
        "TGT_PAD_IDX = EN_VOCAB['<pad>']\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "\n",
        "    # Pad source and target sequences dynamically\n",
        "    src_batch = pad_sequence(src_batch, padding_value=SRC_PAD_IDX, batch_first=True)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=TGT_PAD_IDX, batch_first=True)\n",
        "\n",
        "    return src_batch, tgt_batch\n"
      ],
      "metadata": {
        "id": "jBO40P_bPUkj"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage with Data Loader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# Try a batch\n",
        "src_batch, tgt_batch = next(iter(train_loader))\n",
        "print(src_batch.shape)  # [batch_size, max_src_len]\n",
        "print(tgt_batch.shape)  # [batch_size, max_tgt_len]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0ouXyyMTEh_",
        "outputId": "146c448a-9bdf-40d4-c0e1-432f4e1c91d9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 61])\n",
            "torch.Size([32, 59])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fLBdrPQ5Ectg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 7: Positional encoding\n",
        "#Inject order information into embeddings\n",
        "# Inject position awareness into the model\n",
        "# Use either sinusoidal encoding (from the paper ) or learned encoding\n",
        "# Paper Section 3.5\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "\n",
        "        # Create a long enough positional encoding matrix: [max_len, d_model]\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(\n",
        "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
        "        )\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
        "\n",
        "        pe = pe.unsqueeze(0)  # Shape: [1, max_len, d_model] for broadcasting\n",
        "        self.register_buffer(\"pe\", pe)  # Not a learnable param\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x shape: [batch_size, seq_len, d_model]\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "pD0g1tzD6-3m"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example use\n",
        "# Let's say you have a batch of embeddings (e.g., from token IDs → embedding layer):\n",
        "\"\"\"\n",
        "embed = nn.Embedding(len(HI_VOCAB), d_model)\n",
        "pos_enc = PositionalEncoding(d_model=d_model)\n",
        "\n",
        "x = embed(src_batch)           # shape: [batch_size, seq_len, d_model]\n",
        "x = pos_enc(x)                 # positionally encoded embeddings\n",
        "\n",
        "\"\"\"\n",
        "# register_buffer ensures the position matrix is stored with the model (and moves to CUDA if needed) but doesn’t get updated during backprop.\n",
        "# max_len=5000 is typical; you can increase this if your sentences go longer.\n",
        "# This is purely additive to embeddings."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "SORiF-tsE2Ta",
        "outputId": "5aa1fcd3-8ae5-40ca-faa8-49861ee76450"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nembed = nn.Embedding(len(HI_VOCAB), d_model)\\npos_enc = PositionalEncoding(d_model=d_model)\\n\\nx = embed(src_batch)           # shape: [batch_size, seq_len, d_model]\\nx = pos_enc(x)                 # positionally encoded embeddings\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 8: Transformer model architecture\n",
        "#Encoder, decoder, multi-head attention, masking"
      ],
      "metadata": {
        "id": "O36Bo-FW7B70"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Component                                    | Description                             |\n",
        "| -------------------------------------------- | --------------------------------------- |\n",
        "| 1. `Embedding` + `PositionalEncoding`        | For both source and target              |\n",
        "| 2. `Transformer` from `torch.nn.Transformer` | Main encoder-decoder logic              |\n",
        "| 3. Final `Linear` layer                      | To project decoder output to vocab size |\n"
      ],
      "metadata": {
        "id": "rCXqcyTjW9Yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "src_tok_emb and tgt_tok_emb: Token embeddings\n",
        "positional_encoding: Injects order into the embeddings\n",
        "transformer: Applies multi-head attention & encoder-decoder logic\n",
        "generator: Final linear layer → logits over target vocab\n",
        "\n",
        "\n",
        "batch_first=True makes tensor shapes [batch, seq, feature], which matches your padded batches.\n",
        "\n",
        "We still need to define:\n",
        "src_mask and tgt_mask (for causal masking in decoder)\n",
        "Padding masks (to ignore <pad> tokens)\n",
        "\"\"\"\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.1,\n",
        "                 max_len: int = 5000):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "\n",
        "        # src_tok_emb and tgt_tok_emb: Token embeddings\n",
        "        self.src_tok_emb = nn.Embedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, emb_size)\n",
        "\n",
        "        # positional_encoding: Injects order into the embeddings\n",
        "        self.positional_encoding = PositionalEncoding(emb_size, max_len)\n",
        "\n",
        "        # transformer: Applies multi-head attention & encoder-decoder logic\n",
        "        self.transformer = nn.Transformer(d_model=emb_size,\n",
        "                                          nhead=nhead,\n",
        "                                          num_encoder_layers=num_encoder_layers,\n",
        "                                          num_decoder_layers=num_decoder_layers,\n",
        "                                          dim_feedforward=dim_feedforward,\n",
        "                                          dropout=dropout,\n",
        "                                          batch_first=True)\n",
        "\n",
        "        # generator: Final linear layer → logits over target vocab\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask,\n",
        "                src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
        "\n",
        "        # src, tgt shape: [batch_size, seq_len]\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt))\n",
        "\n",
        "        output = self.transformer(src_emb, tgt_emb,\n",
        "                                  src_mask=src_mask,\n",
        "                                  tgt_mask=tgt_mask,\n",
        "                                  src_key_padding_mask=src_padding_mask,\n",
        "                                  tgt_key_padding_mask=tgt_padding_mask,\n",
        "                                  memory_key_padding_mask=memory_key_padding_mask)\n",
        "\n",
        "        return self.generator(output)\n"
      ],
      "metadata": {
        "id": "kFYNkizwXDB8"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 9: Loss function and masks\n",
        "#Padding mask and look-ahead mask in decoder\n",
        "\n",
        "# Causal Mask (Target Mask)\n",
        "# For autoregressive decoding — at step t, we can only see tokens <= t.\n",
        "# Look Ahead Mask : This is applied to the target (tgt) sequence inside the decoder self-attention.\n",
        "\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    return torch.triu(torch.ones(sz, sz, dtype=torch.bool), diagonal=1)\n",
        "\n",
        "\n",
        "# Padding mask\n",
        "\"\"\"\n",
        "This creates a mask that’s:\n",
        "\n",
        "- 1 for real tokens\n",
        "- 0 for <pad> tokens\n",
        "\n",
        "Can be broadcasted in attention computations\n",
        "\n",
        "We need this for:\n",
        "\n",
        " - encoder self-attention\n",
        " - decoder cross-attention\n",
        " - sometimes decoder self-attention (combined with look-ahead)\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "IJNy5egr7HfZ",
        "outputId": "1c751f92-79fe-41ba-a42e-acf4f968ee6a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThis creates a mask that’s:\\n\\n- 1 for real tokens\\n- 0 for <pad> tokens\\n\\nCan be broadcasted in attention computations\\n\\nWe need this for:\\n\\n - encoder self-attention\\n - decoder cross-attention\\n - sometimes decoder self-attention (combined with look-ahead)\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Test Code: Example Use of Masking During Forward Pass\n",
        "\n",
        "SRC_VOCAB_SIZE = len(HI_VOCAB)\n",
        "TGT_VOCAB_SIZE = len(EN_VOCAB)\n",
        "EMB_SIZE = 512\n",
        "NHEAD = 8\n",
        "FFN_HID_DIM = 512\n",
        "NUM_ENCODER_LAYERS = 6\n",
        "NUM_DECODER_LAYERS = 6\n",
        "\n",
        "model = Seq2SeqTransformer(\n",
        "    NUM_ENCODER_LAYERS,\n",
        "    NUM_DECODER_LAYERS,\n",
        "    EMB_SIZE,\n",
        "    NHEAD,\n",
        "    SRC_VOCAB_SIZE,\n",
        "    TGT_VOCAB_SIZE,\n",
        "    FFN_HID_DIM\n",
        ").to(device)\n",
        "\n",
        "\n",
        "\n",
        "# # Example vocab: Syntheic Data\n",
        "# # <pad>: 0, <bos>: 1, <eos>: 2, I: 3, love: 4, cats: 5, dogs: 6\n",
        "\n",
        "# pad_idx = 0\n",
        "\n",
        "# # src: \"I love dogs <pad>\"\n",
        "# # tgt: \"<bos> I love cats\"\n",
        "\n",
        "# src = torch.tensor([[3, 4, 6, 0]], device=device)  # e.g., \"I love dogs <pad>\"\n",
        "# tgt = torch.tensor([[1, 3, 4, 5]], device=device)  # e.g., \"<bos> I love cats\"\n",
        "\n",
        "# pad_idx = HI_VOCAB['<pad>']\n",
        "\n",
        "# # Padding masks (shape: [batch_size, seq_len])\n",
        "# src_padding_mask = (src == pad_idx)\n",
        "# tgt_padding_mask = (tgt == pad_idx)\n",
        "\n",
        "# # Look-ahead mask (shape: [tgt_seq_len, tgt_seq_len])\n",
        "# tgt_mask = generate_square_subsequent_mask(tgt.size(1)).to(device)\n",
        "\n",
        "# # No src mask typically, so pass None or a zeros mask\n",
        "# src_mask = None\n",
        "\n",
        "# # memory_key_padding_mask = same as src_padding_mask\n",
        "# output = model(\n",
        "#     src,\n",
        "#     tgt,\n",
        "#     src_mask=src_mask,\n",
        "#     tgt_mask=tgt_mask,\n",
        "#     src_padding_mask=src_padding_mask,\n",
        "#     tgt_padding_mask=tgt_padding_mask,\n",
        "#     memory_key_padding_mask=src_padding_mask\n",
        "# )\n"
      ],
      "metadata": {
        "id": "jRYlj-Ug-BD0"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEk3-4WkKx5z",
        "outputId": "e16960a0-f805-4f9d-bb38-a37054d94dac"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2SeqTransformer(\n",
              "  (src_tok_emb): Embedding(25000, 512)\n",
              "  (tgt_tok_emb): Embedding(25000, 512)\n",
              "  (positional_encoding): PositionalEncoding()\n",
              "  (transformer): Transformer(\n",
              "    (encoder): TransformerEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): TransformerDecoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-5): 6 x TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (multihead_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "          (dropout3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (generator): Linear(in_features=512, out_features=25000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Test mask\n",
        "# generate_square_subsequent_mask(4)\n",
        "\n",
        "# # tensor([\n",
        "# # [0, -inf, -inf, -inf],\n",
        "# # [0,   0, -inf, -inf],\n",
        "# # [0,   0,   0, -inf],\n",
        "# # [0,   0,   0,   0]])"
      ],
      "metadata": {
        "id": "0AL9jkneWbq9"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Step 10: Training loop\n",
        "#Batching, optimizer, loss, learning rate schedule\n",
        "\"\"\"\n",
        "nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "Optimizer (Adam)\n",
        "Learning rate scheduler (optional)\n",
        "Forward + loss computation\n",
        "Backward + optimizer step\n",
        "Logging every N batches\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Lic_EZuF7LG5",
        "outputId": "b0aa1cd0-5379-4959-b7d8-25d81fa0e7fb"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnn.CrossEntropyLoss(ignore_index=PAD_IDX)\\nOptimizer (Adam)\\nLearning rate scheduler (optional)\\nForward + loss computation\\nBackward + optimizer step\\nLogging every N batches\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss function (ignore padding index)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=TGT_PAD_IDX)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Mask generators\n",
        "def create_masks(src, tgt_input):\n",
        "    src_mask = None\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
        "\n",
        "    seq_len = tgt_input.size(1)\n",
        "    tgt_mask = generate_square_subsequent_mask(seq_len).to(device)\n",
        "\n",
        "    src_padding_mask = (src == SRC_PAD_IDX)\n",
        "    tgt_padding_mask = (tgt_input == TGT_PAD_IDX)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n"
      ],
      "metadata": {
        "id": "rBN6sXX9Mvbg"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, data_loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for src, tgt in data_loader:\n",
        "        src = src.to(device)\n",
        "        tgt = tgt.to(device)\n",
        "\n",
        "        # Prepare inputs/outputs\n",
        "        tgt_input = tgt[:, :-1]    # input to decoder\n",
        "        tgt_out = tgt[:, 1:]       # prediction target\n",
        "\n",
        "        # Generate masks\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_masks(src, tgt_input)\n",
        "\n",
        "        # Forward\n",
        "        logits = model(\n",
        "            src, tgt_input,\n",
        "            src_mask=src_mask,\n",
        "            tgt_mask=tgt_mask,\n",
        "            src_padding_mask=src_padding_mask,\n",
        "            tgt_padding_mask=tgt_padding_mask,\n",
        "            memory_key_padding_mask=src_padding_mask\n",
        "        )\n",
        "\n",
        "        # Flatten outputs and targets\n",
        "        logits = logits.reshape(-1, logits.size(-1))\n",
        "        tgt_out = tgt_out.reshape(-1)\n",
        "\n",
        "        loss = criterion(logits, tgt_out)\n",
        "\n",
        "        # Backprop\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(data_loader)\n"
      ],
      "metadata": {
        "id": "YfsuHXljNzT_"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model and optimizer state dicts\n",
        "def save_checkpoint(model, optimizer, epoch, path=\"checkpoint.pth\"):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict()\n",
        "    }, path)\n",
        "    print(f\"Checkpoint saved at epoch {epoch} to {path}\")\n",
        "\n",
        "# Load model and optimizer state dicts\n",
        "def load_checkpoint(model, optimizer, path=\"checkpoint.pth\"):\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1\n",
        "    print(f\"Checkpoint loaded. Resuming from epoch {start_epoch}\")\n",
        "    return start_epoch\n"
      ],
      "metadata": {
        "id": "UaZXBok7jTGx"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n",
        "    print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "# Save checkpoint once after training completes\n",
        "torch.save({\n",
        "    'epoch': EPOCHS,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict()\n",
        "}, \"last_checkpoint.pth\")\n",
        "\n",
        "print(\"Training complete. Checkpoint saved.\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7keht_4fN2dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from google.colab import files\n",
        "\n",
        "# After training finishes\n",
        "checkpoint_path = \"last_checkpoint.pth\"\n",
        "torch.save({\n",
        "    'epoch': EPOCHS,\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict()\n",
        "}, checkpoint_path)\n",
        "\n",
        "# Download to local machine\n",
        "files.download(checkpoint_path)\n"
      ],
      "metadata": {
        "id": "H-QMtHY8lABZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Greedy decoding:\n",
        "# Starts with <bos>\n",
        "# Predicts one token at a time\n",
        "# Feeds each predicted token back into the decoder until it generates <eos> or hits max length\n",
        "\n",
        "\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol, src_padding_mask):\n",
        "    model.eval()\n",
        "\n",
        "    src = src.to(device)\n",
        "    src_mask = src_mask\n",
        "    src_padding_mask = src_padding_mask\n",
        "\n",
        "    memory = model.transformer.encoder(\n",
        "        model.positional_encoding(model.src_tok_emb(src)),\n",
        "        src_key_padding_mask=src_padding_mask\n",
        "    )\n",
        "\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
        "\n",
        "    for i in range(max_len - 1):\n",
        "        tgt_mask = generate_square_subsequent_mask(ys.size(1)).to(device)\n",
        "\n",
        "        out = model.transformer.decoder(\n",
        "            model.positional_encoding(model.tgt_tok_emb(ys)),\n",
        "            memory,\n",
        "            tgt_mask=tgt_mask,\n",
        "            memory_key_padding_mask=src_padding_mask\n",
        "        )\n",
        "\n",
        "        out = model.generator(out[:, -1])\n",
        "        next_token = torch.argmax(out, dim=-1).item()\n",
        "\n",
        "        ys = torch.cat([ys, torch.ones(1, 1).fill_(next_token).type(torch.long).to(device)], dim=1)\n",
        "\n",
        "        if next_token == EOS_IDX:\n",
        "            break\n",
        "\n",
        "    return ys\n"
      ],
      "metadata": {
        "id": "Luu6-qluN-52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def beam_search_decode(model, src, src_mask, max_len, start_symbol, src_padding_mask, beam_width=3):\n",
        "    model.eval()\n",
        "\n",
        "    # Encode source\n",
        "    # src shape: [batch_size, src_len]\n",
        "    memory = model.transformer.encoder(\n",
        "        model.positional_encoding(model.src_tok_emb(src)),\n",
        "        src_key_padding_mask=src_padding_mask\n",
        "    ) # memory shape: [batch_size, src_len, d_model]\n",
        "\n",
        "\n",
        "    # Initialize beams: list of (sequence_tensor, score)\n",
        "    # sequence_tensor shape: [seq_len] for each beam initially\n",
        "    beams = [(torch.tensor([start_symbol], dtype=torch.long, device=device), 0)]\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        candidates = []\n",
        "        for seq, score in beams:\n",
        "            if seq[-1].item() == EOS_IDX:  # If EOS, keep the beam\n",
        "                candidates.append((seq, score))\n",
        "                continue\n",
        "\n",
        "            # Prepare target input for the decoder\n",
        "            # seq shape: [current_seq_len]\n",
        "            # tgt_input shape: [batch_size=1, current_seq_len, d_model]\n",
        "            tgt_input = model.positional_encoding(model.tgt_tok_emb(seq.unsqueeze(0))) # Add batch dimension\n",
        "\n",
        "\n",
        "            # Generate causal mask for the decoder self-attention\n",
        "            # tgt_mask shape: [current_seq_len, current_seq_len]\n",
        "            tgt_mask = generate_square_subsequent_mask(seq.size(0)).to(device)\n",
        "\n",
        "            # memory_key_padding_mask shape: [batch_size=1, src_len]\n",
        "            # This is the same as src_padding_mask\n",
        "\n",
        "            out = model.transformer.decoder(\n",
        "                tgt_input, # shape: [1, current_seq_len, d_model]\n",
        "                memory,    # shape: [1, src_len, d_model]\n",
        "                tgt_mask=tgt_mask, # shape: [current_seq_len, current_seq_len]\n",
        "                memory_key_padding_mask=src_padding_mask # shape: [1, src_len]\n",
        "            ) # out shape: [1, current_seq_len, d_model]\n",
        "\n",
        "\n",
        "            # Get logits for the last generated token\n",
        "            logits = model.generator(out[:, -1])  # shape: [1, vocab_size]\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "            topk_log_probs, topk_indices = log_probs.topk(beam_width)\n",
        "\n",
        "            for i in range(beam_width):\n",
        "                next_token = topk_indices[0, i]\n",
        "                next_score = score + topk_log_probs[0, i].item()\n",
        "                next_seq = torch.cat([seq, next_token.unsqueeze(0)])\n",
        "                candidates.append((next_seq, next_score))\n",
        "\n",
        "        # Keep top k beams based on scores\n",
        "        beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "\n",
        "    # Return the best sequence (highest score)\n",
        "    return beams[0][0]"
      ],
      "metadata": {
        "id": "b8dZes47SfWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_beam_search(model, src_sentence, beam_width=3, max_len=50):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize and encode source sentence\n",
        "    tokens = HI_TOKENIZER(src_sentence)\n",
        "    src_indices = [HI_VOCAB['<bos>']] + [HI_VOCAB[token] for token in tokens] + [HI_VOCAB['<eos>']]\n",
        "    src_tensor = torch.tensor(src_indices, dtype=torch.long, device=device).unsqueeze(0) # Add batch dimension\n",
        "\n",
        "    src_padding_mask = (src_tensor == SRC_PAD_IDX)\n",
        "\n",
        "    src_mask = None  # or create if your model needs\n",
        "\n",
        "    with torch.no_grad():\n",
        "        decoded_ids = beam_search_decode(\n",
        "            model,\n",
        "            src_tensor,\n",
        "            src_mask,\n",
        "            max_len,\n",
        "            BOS_IDX,\n",
        "            src_padding_mask,\n",
        "            beam_width\n",
        "        ).flatten()\n",
        "\n",
        "    # Remove special tokens from output and convert to text\n",
        "    tokens = [EN_VOCAB.lookup_token(idx) for idx in decoded_ids if idx not in {BOS_IDX, EOS_IDX, PAD_IDX}]\n",
        "\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "C9uTG8gKORsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(model, src_sentence, beam_width=3, max_len=50):\n",
        "    model.eval()\n",
        "\n",
        "    tokens = HI_TOKENIZER(src_sentence)\n",
        "    src_indices = [HI_VOCAB['<bos>']] + [HI_VOCAB[token] for token in tokens] + [HI_VOCAB['<eos>']]\n",
        "\n",
        "    src_tensor = torch.tensor(src_indices, dtype=torch.long, device=device).unsqueeze(0)  # batch first: (1, seq_len)\n",
        "    #src_padding_mask = (src_tensor == SRC_PAD_IDX)  # boolean tensor\n",
        "    src_padding_mask = (src_tensor == SRC_PAD_IDX)  # Make sure it's 2D: [batch_size, src_len]\n",
        "    if src_padding_mask.dim() == 1:\n",
        "        src_padding_mask = src_padding_mask.unsqueeze(0)\n",
        "\n",
        "    src_mask = None\n",
        "    # print(f\"src_indices = {src_indices}\")\n",
        "    # print(f\"src_tensor.shape = {src_tensor.shape}\")\n",
        "    # print(f\"src_padding_mask.shape = {src_padding_mask.shape}\")\n",
        "    with torch.no_grad():\n",
        "        decoded_ids = beam_search_decode(\n",
        "            model,\n",
        "            src_tensor,\n",
        "            src_mask,\n",
        "            max_len,\n",
        "            BOS_IDX,\n",
        "            src_padding_mask,\n",
        "            beam_width\n",
        "        ).flatten()\n",
        "\n",
        "    tokens = [EN_VOCAB.lookup_token(idx) for idx in decoded_ids if idx not in {BOS_IDX, EOS_IDX, PAD_IDX}]\n",
        "    return \" \".join(tokens)"
      ],
      "metadata": {
        "id": "FsqwUuExSk5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(model.eval)\n",
        "print(translate(model, \"भारत एक सुंदर देश है\"))\n",
        "# Output might be random initially unless trained\n"
      ],
      "metadata": {
        "id": "XgFEwfZnOenV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}